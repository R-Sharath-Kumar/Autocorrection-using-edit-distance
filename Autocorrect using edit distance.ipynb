{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the corpus: Jungle Book in this case\n",
    "\n",
    "file_name = 'frankenstein.txt'\n",
    "\n",
    "with open(file_name, 'r',errors='ignore') as file:\n",
    "    all_words = file.read().lower()\n",
    "    words = re.findall('\\w+',all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ï',\n",
       " 'the',\n",
       " 'project',\n",
       " 'gutenberg',\n",
       " 'ebook',\n",
       " 'of',\n",
       " 'frankenstein',\n",
       " 'by',\n",
       " 'mary',\n",
       " 'wollstonecraft']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the data\n",
    "\n",
    "# word_l = []\n",
    "\n",
    "# for word in words:\n",
    "    # word_l.append(nlp(word))\n",
    "    \n",
    "def process(text_list):\n",
    "    \"\"\"Process text function.\n",
    "    Input:\n",
    "        text_list: a list containing a text\n",
    "    Output:\n",
    "        word_l: a list of words containing the processed text\n",
    "    \"\"\"\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "    word_l = []\n",
    "\n",
    "    for word in text_list:\n",
    "        if word not in punctuations:  # remove punctuation\n",
    "            word_l.append(word)\n",
    "\n",
    "    return word_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total text is 79512 long and the first ten words are: \n",
      "['ï', 'the', 'project', 'gutenberg', 'ebook', 'of', 'frankenstein', 'by', 'mary', 'wollstonecraft']\n",
      "There are 7599 unique words in the vocabulary\n"
     ]
    }
   ],
   "source": [
    "word_l = process(words)\n",
    "vocab = set(word_l) # This will be our new vocabulary\n",
    "\n",
    "print(f\"The total text is {len(word_l)} long and the first ten words are: \\n{word_l[:10]}\")\n",
    "print(f\"There are {len(vocab)} unique words in the vocabulary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**That's a large enough corpus and a good enough amount of unique words in the vocabulary to get started**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing a get_count function to get a count of all the unique words in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count(word_list):\n",
    "    word_counts = {}                           # Initializing an empty dictionary to store the word counts\n",
    "    \n",
    "    for word in word_list:\n",
    "        if word in word_counts.keys():\n",
    "            word_counts[word] += 1\n",
    "        else:\n",
    "            word_counts[word] = 1\n",
    "    \n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7599 key values pairs\n",
      "The count for the word 'thee' is 29\n"
     ]
    }
   ],
   "source": [
    "word_counts = get_count(word_l)\n",
    "\n",
    "print(f\"There are {len(word_counts)} key values pairs\")\n",
    "print(f\"The count for the word 'thee' is {word_counts.get('frankenstein',0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Strange?! I expected Frankenstein to appear a lot more times**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the prior probabilities of every word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probs(word_dict):\n",
    "    probabilities = {}                    # Initializing an empty dictionary to store the probabilities\n",
    "    \n",
    "    for word in word_counts.keys():\n",
    "        probabilities[word] = word_counts[word]/sum(word_counts.values())\n",
    "    \n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of probs is 7599\n",
      "P('man') is 0.0016\n"
     ]
    }
   ],
   "source": [
    "probs = get_probs(word_counts)\n",
    "print(f\"Length of probs is {len(probs)}\")\n",
    "print(f\"P('man') is {probs['man']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Manipulation Functions\n",
    "\n",
    "#### We'll be using the Levenshtein distance measure in this exercise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that returns all possible single-deletes of a string\n",
    "\n",
    "def delete_letter(word):\n",
    "    delete_list = []\n",
    "    split_list = []                             # We'll be using this list to store all possible slices of the word\n",
    "    \n",
    "    split_list = [(word[:i], word[i:]) for i in range(len(word) + 1) if word[i:]]\n",
    "    \n",
    "    for left, right in split_list:\n",
    "        delete_list.append(left + right[1:])   # We're deleting one letter for each recursive iteration in this part\n",
    "    \n",
    "    return delete_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The words that are one delete away from 'Frankenstein' are:\n",
      "\n",
      "['rankenstein', 'fankenstein', 'frnkenstein', 'frakenstein', 'franenstein', 'franknstein', 'frankestein', 'frankentein', 'frankensein', 'frankenstin', 'frankensten', 'frankenstei']\n"
     ]
    }
   ],
   "source": [
    "print(f\"The words that are one delete away from 'Frankenstein' are:\\n\\n{delete_letter('frankenstein')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that returns all possible letter-switch words of a string\n",
    "\n",
    "def switch_letter(word):\n",
    "    switch_list = []\n",
    "    split_list = [(word[:i], word[i:]) for i in range(len(word) + 1) if word[i:]]\n",
    "    \n",
    "    for left, right in split_list:\n",
    "        if len(right) >= 2:\n",
    "            new_string = left + right[1] + right[0] + right[2:]\n",
    "            switch_list.append(new_string)\n",
    "            \n",
    "    return switch_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The words that are formed by switching adjacent letters in 'Frankenstein' are:\n",
      "\n",
      "['rfankenstein', 'farnkenstein', 'frnakenstein', 'fraknenstein', 'franeknstein', 'franknestein', 'frankesntein', 'frankentsein', 'frankensetin', 'frankenstien', 'frankensteni']\n"
     ]
    }
   ],
   "source": [
    "print(f\"The words that are formed by switching adjacent letters in 'Frankenstein' are:\\n\\n{switch_letter('frankenstein')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that replaces each letter in the word with all alphabets\n",
    "\n",
    "def replace_letter(word):\n",
    "    alphabet = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "    split_list = [(word[:i], word[i:]) for i in range(len(word) + 1) if word[i:]]\n",
    "    replace_list = []\n",
    "    \n",
    "    for l in alphabet:\n",
    "        for left, right in split_list:\n",
    "            if len(right) > 1:\n",
    "                replace_list.append(left + l + right[1:])\n",
    "            else:\n",
    "                replace_list.append(left + l)\n",
    "    return replace_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The words that are formed by replacing letters in 'can' are:\n",
      "\n",
      "['aan', 'can', 'caa', 'ban', 'cbn', 'cab', 'can', 'ccn', 'cac', 'dan', 'cdn', 'cad', 'ean', 'cen', 'cae', 'fan', 'cfn', 'caf', 'gan', 'cgn', 'cag', 'han', 'chn', 'cah', 'ian', 'cin', 'cai', 'jan', 'cjn', 'caj', 'kan', 'ckn', 'cak', 'lan', 'cln', 'cal', 'man', 'cmn', 'cam', 'nan', 'cnn', 'can', 'oan', 'con', 'cao', 'pan', 'cpn', 'cap', 'qan', 'cqn', 'caq', 'ran', 'crn', 'car', 'san', 'csn', 'cas', 'tan', 'ctn', 'cat', 'uan', 'cun', 'cau', 'van', 'cvn', 'cav', 'wan', 'cwn', 'caw', 'xan', 'cxn', 'cax', 'yan', 'cyn', 'cay', 'zan', 'czn', 'caz']\n"
     ]
    }
   ],
   "source": [
    "print(f\"The words that are formed by replacing letters in 'can' are:\\n\\n{replace_letter('can')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that inserts a letter in a given word at every offset\n",
    "\n",
    "def insert_letter(word):\n",
    "    alphabet = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "    split_list = [(word[:i], word[i:]) for i in range(len(word) + 1)] # Note that this split_list is slightly different from the others\n",
    "    insert_list = []\n",
    "    \n",
    "    for l in alphabet:\n",
    "        for left, right in split_list:\n",
    "            word = left + l + right\n",
    "            insert_list.append(word)\n",
    "    \n",
    "    return insert_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The words that are formed by replacing letters in 'at' are:\n",
      "\n",
      "['aat', 'aat', 'ata', 'bat', 'abt', 'atb', 'cat', 'act', 'atc', 'dat', 'adt', 'atd', 'eat', 'aet', 'ate', 'fat', 'aft', 'atf', 'gat', 'agt', 'atg', 'hat', 'aht', 'ath', 'iat', 'ait', 'ati', 'jat', 'ajt', 'atj', 'kat', 'akt', 'atk', 'lat', 'alt', 'atl', 'mat', 'amt', 'atm', 'nat', 'ant', 'atn', 'oat', 'aot', 'ato', 'pat', 'apt', 'atp', 'qat', 'aqt', 'atq', 'rat', 'art', 'atr', 'sat', 'ast', 'ats', 'tat', 'att', 'att', 'uat', 'aut', 'atu', 'vat', 'avt', 'atv', 'wat', 'awt', 'atw', 'xat', 'axt', 'atx', 'yat', 'ayt', 'aty', 'zat', 'azt', 'atz']\n",
      "\n",
      "\n",
      "Number of strings output by insert_letter('at') is 78\n"
     ]
    }
   ],
   "source": [
    "print(f\"The words that are formed by replacing letters in 'at' are:\\n\\n{insert_letter('at')}\")\n",
    "print('\\n')\n",
    "print(f\"Number of strings output by insert_letter('at') is {len(insert_letter('at'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining all the edits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Words One Edit Away"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_one(word, allow_switches = True):\n",
    "    edit_one = set()\n",
    "    \n",
    "    edit_one.update(delete_letter(word))\n",
    "    if allow_switches:\n",
    "        edit_one.update(switch_letter(word))\n",
    "    edit_one.update(replace_letter(word))\n",
    "    edit_one.update(insert_letter(word))\n",
    "    \n",
    "    return edit_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input word life \n",
      "edit_one_l \n",
      "['aife', 'alife', 'bife', 'blife', 'cife', 'clife', 'dife', 'dlife', 'eife', 'elife', 'fife', 'flife', 'gife', 'glife', 'hife', 'hlife', 'ife', 'iife', 'ilfe', 'ilife', 'jife', 'jlife', 'kife', 'klife', 'lafe', 'laife', 'lbfe', 'lbife', 'lcfe', 'lcife', 'ldfe', 'ldife', 'lefe', 'leife', 'lfe', 'lffe', 'lfie', 'lfife', 'lgfe', 'lgife', 'lhfe', 'lhife', 'liae', 'liafe', 'libe', 'libfe', 'lice', 'licfe', 'lide', 'lidfe', 'lie', 'liee', 'lief', 'liefe', 'lif', 'lifa', 'lifae', 'lifb', 'lifbe', 'lifc', 'lifce', 'lifd', 'lifde', 'life', 'lifea', 'lifeb', 'lifec', 'lifed', 'lifee', 'lifef', 'lifeg', 'lifeh', 'lifei', 'lifej', 'lifek', 'lifel', 'lifem', 'lifen', 'lifeo', 'lifep', 'lifeq', 'lifer', 'lifes', 'lifet', 'lifeu', 'lifev', 'lifew', 'lifex', 'lifey', 'lifez', 'liff', 'liffe', 'lifg', 'lifge', 'lifh', 'lifhe', 'lifi', 'lifie', 'lifj', 'lifje', 'lifk', 'lifke', 'lifl', 'lifle', 'lifm', 'lifme', 'lifn', 'lifne', 'lifo', 'lifoe', 'lifp', 'lifpe', 'lifq', 'lifqe', 'lifr', 'lifre', 'lifs', 'lifse', 'lift', 'lifte', 'lifu', 'lifue', 'lifv', 'lifve', 'lifw', 'lifwe', 'lifx', 'lifxe', 'lify', 'lifye', 'lifz', 'lifze', 'lige', 'ligfe', 'lihe', 'lihfe', 'liie', 'liife', 'lije', 'lijfe', 'like', 'likfe', 'lile', 'lilfe', 'lime', 'limfe', 'line', 'linfe', 'lioe', 'liofe', 'lipe', 'lipfe', 'liqe', 'liqfe', 'lire', 'lirfe', 'lise', 'lisfe', 'lite', 'litfe', 'liue', 'liufe', 'live', 'livfe', 'liwe', 'liwfe', 'lixe', 'lixfe', 'liye', 'liyfe', 'lize', 'lizfe', 'ljfe', 'ljife', 'lkfe', 'lkife', 'llfe', 'llife', 'lmfe', 'lmife', 'lnfe', 'lnife', 'lofe', 'loife', 'lpfe', 'lpife', 'lqfe', 'lqife', 'lrfe', 'lrife', 'lsfe', 'lsife', 'ltfe', 'ltife', 'lufe', 'luife', 'lvfe', 'lvife', 'lwfe', 'lwife', 'lxfe', 'lxife', 'lyfe', 'lyife', 'lzfe', 'lzife', 'mife', 'mlife', 'nife', 'nlife', 'oife', 'olife', 'pife', 'plife', 'qife', 'qlife', 'rife', 'rlife', 'sife', 'slife', 'tife', 'tlife', 'uife', 'ulife', 'vife', 'vlife', 'wife', 'wlife', 'xife', 'xlife', 'yife', 'ylife', 'zife', 'zlife']\n",
      "\n",
      "Number of outputs from edit_one_letter('at') is 234\n"
     ]
    }
   ],
   "source": [
    "example = \"life\"\n",
    "example_set = edit_one(example)\n",
    "example_list = sorted(list(example_set))\n",
    "\n",
    "print(f\"input word {example} \\nedit_one_l \\n{example_list}\\n\")\n",
    "print(f\"Number of outputs from edit_one_letter('at') is {len(edit_one('life'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Words Two Edits Away"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_two(word, allow_switches = True):\n",
    "    edit_two_set = set()\n",
    "    \n",
    "    edit_one_set = edit_one(word,allow_switches=allow_switches)\n",
    "    for w in edit_one_set:\n",
    "        if w:\n",
    "            edit_two = edit_one(w,allow_switches=allow_switches)\n",
    "            edit_two_set.update(edit_two)\n",
    "    \n",
    "    return edit_two_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 strings ['', 'a', 'aa', 'aaa', 'aaat', 'aab', 'aabt', 'aac', 'aact', 'aad']\n",
      "Last 10 strings ['zwt', 'zx', 'zxat', 'zxt', 'zy', 'zyat', 'zyt', 'zz', 'zzat', 'zzt']\n",
      "Number of strings that are 2 edit distances from 'at' is 7154\n"
     ]
    }
   ],
   "source": [
    "example_2 = edit_two(\"at\")\n",
    "example_2_list = sorted(list(example_2))\n",
    "print(f\"First 10 strings {example_2_list[:10]}\")\n",
    "print(f\"Last 10 strings {example_2_list[-10:]}\")\n",
    "print(f\"Number of strings that are 2 edit distances from 'at' is {len(edit_two('at'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suggest Spelling Suggestions for given word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spell_correct(word, probs, vocab, n = 3, verbose = True):\n",
    "    suggestions = []\n",
    "    n_best = []\n",
    "    \n",
    "    suggestions = list((word in vocab and word) or (edit_one(word).intersection(vocab) or (edit_two(word).intersection(vocab))))\n",
    "    def takeSecond(elem):\n",
    "        return elem[1]\n",
    "    n_best = [[s,probs[s]] for s in list(reversed(suggestions))]\n",
    "    n_best.sort(key=takeSecond, reverse=True)\n",
    "    \n",
    "    if verbose: print(f\"the word = {word}  \\nthe suggestions = {n_best}\")\n",
    "    return n_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the word = frankinstein  \n",
      "the suggestions = [['frankenstein', 0.0003647248214106047]]\n",
      "word 1: frankenstein, probability 0.000365\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "\n",
    "my_word = 'frankinstein' # Mispelling of 'Frankenstein'\n",
    "tmp_corrections = spell_correct(my_word, probs, vocab, 2, verbose=True) # keep verbose=True\n",
    "\n",
    "for i, word_prob in enumerate(tmp_corrections):\n",
    "    print(f\"word {i+1}: {word_prob[0]}, probability {word_prob[1]:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the word = deas  \n",
      "the suggestions = [['dear', 0.0008552168226179696], ['dead', 0.00028926451353254853], ['ideas', 0.00017607405171546434], ['seas', 7.546030787805614e-05], ['dews', 2.5153435959352048e-05], ['dens', 1.2576717979676024e-05], ['deal', 1.2576717979676024e-05]]\n",
      "word 1: dear, probability 0.000855\n",
      "word 2: dead, probability 0.000289\n",
      "word 3: ideas, probability 0.000176\n",
      "word 4: seas, probability 0.000075\n",
      "word 5: dews, probability 0.000025\n",
      "word 6: dens, probability 0.000013\n",
      "word 7: deal, probability 0.000013\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "\n",
    "my_word = 'deas' \n",
    "tmp_corrections = spell_correct(my_word, probs, vocab, 2, verbose=True) # keep verbose=True\n",
    "\n",
    "for i, word_prob in enumerate(tmp_corrections):\n",
    "    print(f\"word {i+1}: {word_prob[0]}, probability {word_prob[1]:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closing Notes\n",
    "\n",
    "### The Corpus\n",
    "\n",
    "**The vocabulary and probability scores we got depended on the corpus. In this case we chose Frankestein by Mary Shelley because its available on Project Gutenburg for free. However, if we want to improve the spell corrector, we'd have the choose a different and more general corpus.**\n",
    "\n",
    "### I got the idea for this from a course from the NLP specialisation on Coursera (deeplearning.ai) and the idea we used here is from this article (https://norvig.com/spell-correct.html) by Peter Norvig (2007)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
